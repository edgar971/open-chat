<?xml version="1.0"?>
<Container version="2">
  <Name>OpenChat-Cuda</Name>
  <Repository>ghcr.io/edgar971/open-chat-cuda:v1.0.6</Repository>
  <Registry>ghcr.io/edgar971/open-chat-cuda:v1.0.6</Registry>
  <Network>bridge</Network>
  <MyIP/>
  <Shell>sh</Shell>
  <Privileged>false</Privileged>
  <Support>https://github.com/edgar971/open-chat/issues</Support>
  <Project>https://github.com/edgar971/open-chat</Project>
  <Overview>A self-hosted, offline, ChatGPT-like chatbot with open source LLM support. 100% private, with no data leaving your device.&#xD; Please note that this version requires an NVIDIA GPU with the Unraid NVIDIA-DRIVER plugin. 
</Overview>
  <Category>HomeAutomation: Productivity: Tools:</Category>
  <WebUI>http://[IP]:[PORT:3000]/</WebUI>
  <TemplateURL/>
  <Icon>https://github.com/edgar971/open-chat/raw/main/logo.png</Icon>
  <ExtraParams>--gpus all</ExtraParams>
  <PostArgs/>
  <CPUset/>
  <DateInstalled>1692670577</DateInstalled>
  <DonateText/>
  <DonateLink/>
  <Requires/>
  <Config Name="Local Model Path" Target="MODEL" Default="/models/llama-2-7b-chat.gguf" Mode="" Description="The local model path" Type="Variable" Display="always" Required="true" Mask="false"/>
  <Config Name="Model Download URL" Target="MODEL_DOWNLOAD_URL" Default="https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_0.gguf" Mode="" Description="GGUF Model Binary. " Type="Variable" Display="always" Required="false" Mask="false"/>
  <Config Name="Model Directory" Target="/models" Default="/mnt/user/appdata/models" Mode="rw" Description="The local model directory to use as a cache" Type="Path" Display="always" Required="false" Mask="false"/>
  <Config Name="Web UI" Target="3000" Default="3000" Mode="tcp" Description="Chat UI Port" Type="Port" Display="always" Required="false" Mask="false"/>
  <Config Name="API Port" Target="8000" Default="8000" Mode="tcp" Description="HTTP API Port" Type="Port" Display="always" Required="false" Mask="false"/>
  <Config Name="Number Of GPU Layers" Target="N_GPU_LAYERS" Default="" Mode="" Description="Layers to offload to GPU. Update this number if server fails to load." Type="Variable" Display="always" Required="false" Mask="false">12</Config>
</Container>
